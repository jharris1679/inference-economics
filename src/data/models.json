{
  "updatedAt": "2026-01-18",
  "methodology": "Q4_K_M quantization unless noted, batch=1 decode speed, interactive use case",
  "developers": {
    "openai": {
      "name": "OpenAI",
      "models": {
        "gpt-oss-20b": {
          "name": "gpt-oss-20b",
          "params": "21B",
          "activeParams": "3.6B",
          "minRAM": 16,
          "localTokPerSec": 80,
          "dgxSparkTokPerSec": 86,
          "cloudTokPerSec": 150,
          "cloudGPUs": 0.2,
          "quantization": "MXFP4",
          "tier": "small",
          "notes": "MoE architecture, ~12GB loaded, 5 instances per H100"
        },
        "gpt-oss-120b": {
          "name": "gpt-oss-120b",
          "params": "117B",
          "activeParams": "5.1B",
          "minRAM": 80,
          "localTokPerSec": 50,
          "dgxSparkTokPerSec": 58,
          "cloudTokPerSec": 100,
          "cloudGPUs": 1,
          "quantization": "MXFP4",
          "tier": "large",
          "notes": "MoE, single 80GB H100"
        }
      }
    },
    "meta": {
      "name": "Meta",
      "models": {
        "llama-3.1-8b": {
          "name": "Llama 3.1 8B",
          "params": "8B",
          "minRAM": 8,
          "localTokPerSec": 90,
          "dgxSparkTokPerSec": 45,
          "cloudTokPerSec": 120,
          "cloudGPUs": 0.1,
          "quantization": "Q4_K_M",
          "tier": "small",
          "notes": "~5GB loaded, 10 instances per H100"
        },
        "llama-3.1-70b": {
          "name": "Llama 3.1 70B",
          "params": "70B",
          "minRAM": 48,
          "localTokPerSec": 12,
          "dgxSparkTokPerSec": 6,
          "cloudTokPerSec": 35,
          "cloudGPUs": 2,
          "quantization": "Q4_K_M",
          "tier": "large",
          "notes": "~40GB weights, TP=2"
        },
        "llama-3.1-405b": {
          "name": "Llama 3.1 405B",
          "params": "405B",
          "minRAM": 256,
          "localTokPerSec": 2.5,
          "dgxSparkTokPerSec": 0,
          "cloudTokPerSec": 15,
          "cloudGPUs": 8,
          "quantization": "FP8",
          "tier": "frontier",
          "notes": "~200GB weights, 8Ã— H100"
        },
        "llama-4-scout": {
          "name": "Llama 4 Scout",
          "params": "109B",
          "activeParams": "17B",
          "minRAM": 64,
          "localTokPerSec": 22,
          "dgxSparkTokPerSec": 18,
          "cloudTokPerSec": 135,
          "cloudGPUs": 1,
          "quantization": "Q4_K_M",
          "tier": "large",
          "notes": "MoE 16 experts, 10M context, fits single H100"
        },
        "llama-4-maverick": {
          "name": "Llama 4 Maverick",
          "params": "400B",
          "activeParams": "17B",
          "minRAM": 240,
          "localTokPerSec": 25,
          "dgxSparkTokPerSec": 0,
          "cloudTokPerSec": 55,
          "cloudGPUs": 8,
          "quantization": "Q4_K_M",
          "tier": "frontier",
          "notes": "MoE 128 experts, 1M context, beats GPT-4o"
        }
      }
    },
    "google": {
      "name": "Google",
      "models": {
        "gemma-3-2b": {
          "name": "Gemma 3 2B",
          "params": "2B",
          "minRAM": 4,
          "localTokPerSec": 120,
          "dgxSparkTokPerSec": 60,
          "cloudTokPerSec": 160,
          "cloudGPUs": 0.05,
          "quantization": "Q4_K_M",
          "tier": "small",
          "notes": "~2GB loaded, 20 instances per H100"
        },
        "gemma-3-9b": {
          "name": "Gemma 3 9B",
          "params": "9B",
          "minRAM": 8,
          "localTokPerSec": 75,
          "dgxSparkTokPerSec": 38,
          "cloudTokPerSec": 100,
          "cloudGPUs": 0.1,
          "quantization": "Q4_K_M",
          "tier": "small",
          "notes": "~6GB loaded, 10 instances per H100"
        },
        "gemma-3-27b": {
          "name": "Gemma 3 27B",
          "params": "27B",
          "minRAM": 20,
          "localTokPerSec": 32,
          "dgxSparkTokPerSec": 18,
          "cloudTokPerSec": 50,
          "cloudGPUs": 0.25,
          "quantization": "Q4_K_M",
          "tier": "medium",
          "notes": "~15GB loaded, 4 instances per H100"
        }
      }
    },
    "deepseek": {
      "name": "DeepSeek",
      "models": {
        "deepseek-coder-33b": {
          "name": "DeepSeek Coder 33B",
          "params": "33B",
          "minRAM": 24,
          "localTokPerSec": 26,
          "dgxSparkTokPerSec": 14,
          "cloudTokPerSec": 42,
          "cloudGPUs": 0.25,
          "quantization": "Q4_K_M",
          "tier": "medium",
          "notes": "~18GB loaded, 4 instances per H100"
        },
        "deepseek-v3": {
          "name": "DeepSeek V3",
          "params": "671B",
          "activeParams": "37B",
          "minRAM": 384,
          "localTokPerSec": 20,
          "dgxSparkTokPerSec": 0,
          "cloudTokPerSec": 25,
          "cloudGPUs": 8,
          "quantization": "Q4_K_M",
          "tier": "frontier",
          "notes": "MoE, 671B total / 37B active"
        },
        "deepseek-v3.2": {
          "name": "DeepSeek V3.2",
          "params": "685B",
          "activeParams": "37B",
          "minRAM": 400,
          "localTokPerSec": 22,
          "dgxSparkTokPerSec": 0,
          "cloudTokPerSec": 30,
          "cloudGPUs": 8,
          "quantization": "Q4_K_M",
          "tier": "frontier",
          "notes": "MoE, DSA attention, surpasses GPT-5 on reasoning"
        },
        "deepseek-r1": {
          "name": "DeepSeek R1",
          "params": "671B",
          "activeParams": "37B",
          "minRAM": 384,
          "localTokPerSec": 18,
          "dgxSparkTokPerSec": 0,
          "cloudTokPerSec": 22,
          "cloudGPUs": 8,
          "quantization": "Q4_K_M",
          "tier": "frontier",
          "notes": "MoE reasoning model, o1-level performance"
        },
        "deepseek-r1-32b": {
          "name": "DeepSeek R1 32B",
          "params": "32B",
          "minRAM": 24,
          "localTokPerSec": 28,
          "dgxSparkTokPerSec": 15,
          "cloudTokPerSec": 45,
          "cloudGPUs": 0.25,
          "quantization": "Q4_K_M",
          "tier": "medium",
          "notes": "Distilled from R1, o1-mini level"
        },
        "deepseek-r1-8b": {
          "name": "DeepSeek R1 8B",
          "params": "8B",
          "minRAM": 8,
          "localTokPerSec": 85,
          "dgxSparkTokPerSec": 42,
          "cloudTokPerSec": 110,
          "cloudGPUs": 0.1,
          "quantization": "Q4_K_M",
          "tier": "small",
          "notes": "Distilled from R1, efficient reasoning"
        }
      }
    },
    "alibaba": {
      "name": "Alibaba (Qwen)",
      "models": {
        "qwen2.5-7b": {
          "name": "Qwen2.5 7B",
          "params": "7B",
          "minRAM": 8,
          "localTokPerSec": 85,
          "dgxSparkTokPerSec": 42,
          "cloudTokPerSec": 110,
          "cloudGPUs": 0.1,
          "quantization": "Q4_K_M",
          "tier": "small",
          "notes": "~4GB loaded, 10 instances per H100"
        },
        "qwen2.5-32b": {
          "name": "Qwen2.5 32B",
          "params": "32B",
          "minRAM": 24,
          "localTokPerSec": 28,
          "dgxSparkTokPerSec": 15,
          "cloudTokPerSec": 45,
          "cloudGPUs": 0.25,
          "quantization": "Q4_K_M",
          "tier": "medium",
          "notes": "~18GB loaded, 4 instances per H100"
        },
        "qwen2.5-72b": {
          "name": "Qwen2.5 72B",
          "params": "72B",
          "minRAM": 48,
          "localTokPerSec": 11,
          "dgxSparkTokPerSec": 5,
          "cloudTokPerSec": 33,
          "cloudGPUs": 2,
          "quantization": "Q4_K_M",
          "tier": "large",
          "notes": "~40GB weights, TP=2"
        },
        "qwen3-8b": {
          "name": "Qwen3 8B",
          "params": "8B",
          "minRAM": 8,
          "localTokPerSec": 88,
          "dgxSparkTokPerSec": 44,
          "cloudTokPerSec": 115,
          "cloudGPUs": 0.1,
          "quantization": "Q4_K_M",
          "tier": "small",
          "notes": "Dense, 128K context, hybrid reasoning"
        },
        "qwen3-32b": {
          "name": "Qwen3 32B",
          "params": "32B",
          "minRAM": 24,
          "localTokPerSec": 30,
          "dgxSparkTokPerSec": 16,
          "cloudTokPerSec": 48,
          "cloudGPUs": 0.25,
          "quantization": "Q4_K_M",
          "tier": "medium",
          "notes": "Dense, 128K context, hybrid reasoning"
        },
        "qwen3-235b-a22b": {
          "name": "Qwen3 235B-A22B",
          "params": "235B",
          "activeParams": "22B",
          "minRAM": 150,
          "localTokPerSec": 20,
          "dgxSparkTokPerSec": 0,
          "cloudTokPerSec": 40,
          "cloudGPUs": 4,
          "quantization": "Q4_K_M",
          "tier": "frontier",
          "notes": "MoE, 128K context, hybrid reasoning"
        }
      }
    },
    "moonshot": {
      "name": "Moonshot (Kimi)",
      "models": {
        "kimi-k2": {
          "name": "Kimi K2",
          "params": "1T",
          "activeParams": "32B",
          "minRAM": 64,
          "localTokPerSec": 25,
          "dgxSparkTokPerSec": 20,
          "cloudTokPerSec": 50,
          "cloudGPUs": 2,
          "quantization": "FP8",
          "tier": "frontier",
          "notes": "MoE, 1T total / 32B active, 256k context"
        }
      }
    },
    "defog": {
      "name": "Defog (SQLCoder)",
      "models": {
        "sqlcoder-7b": {
          "name": "SQLCoder 7B",
          "params": "7B",
          "minRAM": 8,
          "localTokPerSec": 88,
          "dgxSparkTokPerSec": 45,
          "cloudTokPerSec": 115,
          "cloudGPUs": 0.1,
          "quantization": "Q4_K_M",
          "tier": "small",
          "notes": "~4GB loaded, 10 instances per H100"
        },
        "sqlcoder-34b": {
          "name": "SQLCoder 34B",
          "params": "34B",
          "minRAM": 24,
          "localTokPerSec": 26,
          "dgxSparkTokPerSec": 14,
          "cloudTokPerSec": 40,
          "cloudGPUs": 0.25,
          "quantization": "Q4_K_M",
          "tier": "medium",
          "notes": "~18GB loaded, 4 instances per H100"
        },
        "sqlcoder-70b": {
          "name": "SQLCoder 70B",
          "params": "70B",
          "minRAM": 48,
          "localTokPerSec": 11,
          "dgxSparkTokPerSec": 5,
          "cloudTokPerSec": 32,
          "cloudGPUs": 2,
          "quantization": "Q4_K_M",
          "tier": "large",
          "notes": "Highest accuracy, text-to-SQL"
        }
      }
    }
  },
  "sources": [
    {
      "id": "llama-cpp-m-series",
      "name": "llama.cpp Apple Silicon Discussion",
      "url": "https://github.com/ggml-org/llama.cpp/discussions/4167"
    },
    {
      "id": "llama-cpp-dgx-spark",
      "name": "llama.cpp DGX Spark Discussion",
      "url": "https://github.com/ggml-org/llama.cpp/discussions/16578"
    },
    {
      "id": "dlewis-h100-eval",
      "name": "H100 vs A100 Llama 3.3 70B Evaluation",
      "url": "https://dlewis.io/evaluating-llama-33-70b-inference-h100-a100/"
    },
    {
      "id": "hardware-corner-deepseek",
      "name": "DeepSeek V3 Mac Studio Benchmarks",
      "url": "https://www.hardware-corner.net/studio-m3-ultra-running-deepseek-v3/"
    },
    {
      "id": "macrumors-deepseek-r1",
      "name": "DeepSeek R1 on M3 Ultra",
      "url": "https://www.macrumors.com/2025/03/17/apples-m3-ultra-runs-deepseek-r1-efficiently/"
    },
    {
      "id": "nvidia-gpt-oss",
      "name": "NVIDIA gpt-oss Acceleration",
      "url": "https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/"
    },
    {
      "id": "openai-gpt-oss",
      "name": "OpenAI gpt-oss Introduction",
      "url": "https://openai.com/index/introducing-gpt-oss/"
    },
    {
      "id": "moonshot-kimi-k2",
      "name": "Kimi K2 Announcement",
      "url": "https://moonshotai.github.io/Kimi-K2/"
    },
    {
      "id": "valdi-h100",
      "name": "VALDI H100 Performance Docs",
      "url": "https://docs.valdi.ai/llms/performance/gpu/H100/llama3.1-inference-testing/"
    },
    {
      "id": "deepseek-v3.2-release",
      "name": "DeepSeek V3.2 Release Notes",
      "url": "https://api-docs.deepseek.com/news/news251201"
    },
    {
      "id": "deepseek-r1-github",
      "name": "DeepSeek R1 GitHub",
      "url": "https://github.com/deepseek-ai/DeepSeek-R1"
    },
    {
      "id": "llama-4-blog",
      "name": "Meta Llama 4 Announcement",
      "url": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/"
    },
    {
      "id": "hardware-corner-llama4",
      "name": "Llama 4 Mac Studio Benchmarks",
      "url": "https://www.hardware-corner.net/llama-4-on-mac-m3-ultra-speed/"
    },
    {
      "id": "nvidia-llama4",
      "name": "NVIDIA Llama 4 Inference Benchmarks",
      "url": "https://developer.nvidia.com/blog/nvidia-accelerates-inference-on-meta-llama-4-scout-and-maverick/"
    },
    {
      "id": "qwen3-github",
      "name": "Qwen3 GitHub",
      "url": "https://github.com/QwenLM/Qwen3"
    },
    {
      "id": "macstories-qwen3",
      "name": "Qwen3 235B Mac Studio Benchmarks",
      "url": "https://www.macstories.net/notes/notes-on-early-mac-studio-ai-benchmarks-with-qwen3-235b-a22b-and-qwen2-5-vl-72b/"
    }
  ]
}
